{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec3f8e6",
   "metadata": {},
   "source": [
    "# Bellman Abstract Representation Kit - BARK Design\n",
    "\n",
    "_Notebook by Sebastian Benthall_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f24cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from typing import Any, Callable, Mapping, Sequence\n",
    "import xarray as xr\n",
    "\n",
    "from HARK import distribution\n",
    "from HARK.rewards import CRRAutility, CRRAutilityP, CRRAutility_inv, CRRAutilityP_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb454b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HARK.stage import Stage, backwards_induction, simulate_stage\n",
    "\n",
    "import cons_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d730d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doing this because of the CRRAutility warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b213f1",
   "metadata": {},
   "source": [
    "**TODO: Use language of 'blocks'.**\n",
    "\n",
    "**TODO: The shifted Bellman equation here.**\n",
    "\n",
    "**TODO: Other notes from the PDF, which I lost earlier...**\n",
    "\n",
    "**TODO: Work in 'steps' and $\\underline{v}$**\n",
    "\n",
    "This notebook demonstrates HARK's ability to represent and compose Bellman stages.\n",
    "This is possible because all Bellman stages have a general form.\n",
    "\n",
    "In each Bellman stage $S = (\\vec{X}, P_\\vec{K}, \\vec{A}, \\Gamma, F, \\vec{Y}, T, B)$, the agent:\n",
    " - begins in some input states $\\vec{x} \\in \\vec{X}$\n",
    " - experiences some exogeneous shocks $\\vec{k} \\in \\vec{K}$ according to distribution $P_\\vec{K}$\n",
    " - can choose some actions $\\vec{a} \\in \\vec{A}$\n",
    " - subject to constraints $\\Gamma: \\vec{X} \\times \\vec{K} \\rightarrow \\mathcal{P}(\\vec{A})$\n",
    "     - For scalar actions, these may be expressed as upper and lower bounds, such that $\\Gamma_{lb} \\leq a \\leq \\Gamma_{ub}$:\n",
    "         - $\\Gamma_{ub}: \\vec{X} \\times \\vec{K} \\rightarrow \\mathbb{R}$\n",
    "         - $\\Gamma_{lb}: \\vec{X} \\times \\vec{K} \\rightarrow \\mathbb{R}$\n",
    "         - such that $\\Gamma(\\vec{x}, \\vec{k}) = [\\Gamma_{lb}(\\vec{x}, \\vec{k}), \\Gamma_{ub}(\\vec{x}, \\vec{k})]$\n",
    " - experience a reward $F: \\vec{X} \\times \\vec{K} \\times \\vec{A} \\rightarrow \\mathbb{R}$\n",
    " - together, these determine some output states $\\vec{y} \\in \\vec{Y}$ via...\n",
    " - a **deterministic** transition function $T: \\vec{X} \\times \\vec{K} \\times \\vec{A} \\rightarrow \\vec{Y}$\n",
    "   - _This is deterministic because shocks have been isolated to the beginning of the stage._\n",
    " - The agent has a discount factor B for future utility.\n",
    "     - This is often a constant, such as $\\beta$.\n",
    "     - but it can also be a function $B: \\vec{X} \\times \\vec{K} \\times \\vec{A} \\rightarrow \\mathbb{R}$\n",
    "     \n",
    "**TODO**: Notation.\n",
    " - Allow an $h$ operation which is more general than scalar discount factors.\n",
    " - Rename shocks to $Z$, with evidence from Stachurski and Sargent.\n",
    " - rename transition equation to $g$? unless something better from sargent.\n",
    "     \n",
    "### Grids\n",
    "\n",
    "In practice, we will discretized versions of the stage. We will use bold-faced, non-italic $\\mathbf{X}$, $\\mathbf{K}$, $\\mathbf{A}$, and $\\mathbf{Y}$ for grids over the input, shock, action, and output spaces.\n",
    "\n",
    "Note that the shock space $\\mathbf{K}$ will normally be generated by discretizing the continuous probability distribution $P_\\vec{K}$. We will refer to the discretized probability distribution over $\\mathbf{K}$ that preserves point mass values as $\\mathbf{P_\\vec{K}}$\n",
    "\n",
    "**TODO: Use grave mark for interpolated functions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e88bca",
   "metadata": {},
   "source": [
    "## Solving one stage \n",
    "\n",
    "### Policy Optimization\n",
    "\n",
    "Given a stage $S$, we often want to know the optimal policy or decision rule $\\pi^*(\\vec{x}, \\vec{k})= \\vec{a}^*_{xk}$ that yields the best choice of action given input states and shock realizations.\n",
    "\n",
    "There are several different techniques available for policy optimization. In general, the more one is able to provide analytic information about the functions in the stage definition, the better a policy optimization algorithm one can employ.\n",
    "\n",
    "| Method                 | Requirements           | Discretization    | Value Input | Computation  | Products*  |\n",
    "| ---------------------  |:--------------         | :-------------    | :---------- | :----------  | :--------  |\n",
    "| Value Optimization     |                        | $$\\mathbf{X, K}$$ | $$v_y$$     | Optimization | $\\pi^*, q$ |\n",
    "| First Order Condition  | $F', T'$, $B' = 0$     | $$\\mathbf{X, K}$$ | $$v'_y$$    | Rootfinding  | $\\pi^*$    |\n",
    "| Endogenous Gridpoints  | $F'^{-1}, T'$, $B' = 0$| $$\\mathbf{Y} $$   | $$v'_y$$    | None         | $\\pi_y^*$  |     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e187569d",
   "metadata": {},
   "source": [
    "#### Value optimization\n",
    "\n",
    "Given the output value function $v_y : \\vec{Y} \\rightarrow \\mathbb{R}$, the action-value function $q$ is defined as:\n",
    "\n",
    "$$q(\\vec{x}, \\vec{k}, \\vec{a}) = F(\\vec{x}, \\vec{k}, \\vec{a}) + B(\\vec{x},\\vec{k},\\vec{a}) v_y(T(\\vec{x}, \\vec{k}, \\vec{a}))$$\n",
    "\n",
    "This can be computed for all points on the grids $\\mathbf{X, K}$.\n",
    "\n",
    "The optimal policy $\\pi: \\vec{X} \\times \\vec{K} \\rightarrow \\vec{A}$ is:\n",
    "\n",
    "$$\\pi^*(\\vec{x}, \\vec{k}) = \\underset{\\vec{a} \\in \\Gamma(\\vec{x}, \\vec{k})}{\\mathrm{argmax}} q(\\vec{x}, \\vec{k}, \\vec{a})$$\n",
    "\n",
    "(This corresponds to Equation 3 in the notes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a3263",
   "metadata": {},
   "source": [
    "#### First Order Condition\n",
    "\n",
    "Mathematically, the optimization step above truly depends on the marginal value function $v'_y$, and not on $v_y$ (i.e., the solution is indifferent to an additive constant on $v_y$). Optimizing an interpolated value function can can introduce errors. Sometimes, one can get improved results by starting from an interpolatd marginal value function $v'_y$.\n",
    " \n",
    "Given:\n",
    "\n",
    " - $v'_y : Y \\rightarrow \\mathbb{R}$ is the marginal value of output states.\n",
    " - A marginal reward function $F' = \\frac{\\partial F}{\\partial \\vec{a}}$\n",
    " - A marginal transition function $T' = \\frac{\\partial T}{\\partial \\vec{a}}$\n",
    " - A discount factor $B$ such that $B' = \\frac{\\partial B}{\\partial a} = 0$, i.e. because it is constant.\n",
    " \n",
    "Assuming the $q$ function is concave, then the optimal $\\pi^*(\\vec{x}, \\vec{k}) \\in \\vec{A}$ will satisfy the first order condition (FOC). This condition is that the marginal action value function $q' =  \\frac{\\partial q}{\\partial \\vec{a}}$ is 0:\n",
    "\n",
    "$$0 = q'(\\vec{x}, \\vec{k}, \\vec{a})$$\n",
    "\n",
    "$$0 = F'(\\vec{x}, \\vec{k}, \\vec{a}) + B(\\vec{x},\\vec{k},\\vec{a}) v_y'(T(\\vec{x}, \\vec{k}, \\vec{a}))T'(\\vec{x}, \\vec{k}, \\vec{a})$$\n",
    "\n",
    "(This condition is more complex if $B$ depends on the actions because of the Product Rule of differentiation.)\n",
    "\n",
    "This can be computed for all points on the grids $\\mathbf{X, K}$.\n",
    "\n",
    "If $q'(\\vec{x}, \\vec{k}, \\Gamma_{lb}(\\vec{x}, \\vec{k}))$ and $q'(\\vec{x}, \\vec{k}, \\Gamma_{ub}(\\vec{x}, \\vec{k}))$ have the same sign, then the constraints bind, and\n",
    "- $\\pi^*(\\vec{x}, \\vec{k}) = \\Gamma_{lb}(\\vec{x}, \\vec{k})$ (negative sign)\n",
    "- or $\\pi^*(\\vec{x}, \\vec{k}) = \\Gamma_{ub}(\\vec{x}, \\vec{k})$ (positive sign).\n",
    "\n",
    "**TODO: Nice LaTeX braces and cases for this?**\n",
    "\n",
    "Computationally, this involves replacing the numerical optimization step with a numerical root-finding step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0d2f4",
   "metadata": {},
   "source": [
    "#### Endogenous Gridpoints\n",
    "\n",
    "Sometimes, no computation is needed at all to compute the optimal policy! With a catch.\n",
    "\n",
    "Given:\n",
    " - $v'_y : Y \\rightarrow \\mathbb{R}$ is the marginal value of output states.\n",
    " - A constant marginal transition function $T' = \\frac{\\partial T}{\\partial \\vec{a}}$. Note: Some researchers are discovering how to lift this condition!\n",
    " - An inverse marginal reward function $F'^{-1} : \\mathbb{R} \\rightarrow \\vec{A}$.\n",
    " - A constant discount factor $B$ such that $B' = \\frac{\\partial B}{\\partial a} = 0$, i.e. because it is constant.\n",
    " - A grid $\\mathbf{Y}$ over output states.\n",
    "\n",
    "Then we can derive from the FOC that:\n",
    "\n",
    "$$0 = F'(\\vec{x}, \\vec{k}, \\vec{a}) + B T'(\\vec{x}, \\vec{k}, \\vec{a}) v_y'(T(\\vec{x}, \\vec{k}, \\vec{a}))$$\n",
    "\n",
    "$$F'(\\vec{x},\\vec{k},\\pi^*_y(\\vec{y})) = - B T' v_y'(\\vec{y}) $$\n",
    "\n",
    "$$\\pi^*_y(\\vec{y})  =  F'^{-1}(- B T' v_y'(\\vec{y}))$$\n",
    "\n",
    "Unlike the other policy optimizing methods, this is computed over a grid over outputs $\\mathbf{Y}$ and requires no numerical searching over the action space. This takes $O(|Y|)$ time, but with a very small constant.\n",
    "\n",
    "Note that this produces the function $\\pi^*_y : \\vec{Y} \\rightarrow \\vec{A}$, which chooses the optimal action for a given _output_. This is part of the Endogenous Gridpoints Method (Carroll, 2006), so called because it implies an endogenous grid over actions $\\mathbf{A}^* = \\pi^*_y(\\mathbf{Y})$.\n",
    "\n",
    "Under special conditions, this function $\\pi^*_y$ can be used to efficiently compute the input value function.\n",
    "\n",
    "**TODO: What happens if $\\vec{y}$ is multidimensional? Is it e.g. $\\frac{\\partial v_y'(\\vec{y})}{\\partial y_1 \\partial y_2}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf37f4",
   "metadata": {},
   "source": [
    "### Value Backup Methods\n",
    "\n",
    "When solving a problem with backwards induction, we will want to derive input value function $v_x$ from the output value function $v_y$. This requires solving for the optimal policy, discussed above. Once this is in hand, several methods are available.\n",
    "\n",
    "| Method                  | Requirements  | Discretization      | Policy      | Value    | Products*  |\n",
    "| ---------------------   |:------------- | :-------------      | :---------  | :------  | :--------  |\n",
    "| Value Update            |               | $$\\mathbf{X, P_K}$$ | $$\\pi^*$$   | $$v_y$$  | $v_x, q$   |\n",
    "| Analytic Marginal Value | $T'_x, T'_a, $| $$\\mathbf{X, P_K}$$ | $$\\pi^*$$   | $$v'_y$$ | $v'_x$     |\n",
    "| Endogenous Gridpoints?  | $T^{-1}_a$    | $$\\mathbf{Y} $$     | $$\\pi^*_y$$ | $$v'_y$$ | $v'_x$     |  \n",
    "\n",
    "\n",
    "#### Basic value backup\n",
    "\n",
    "With the optimal policy $\\pi^*$ in hand, it is possible to compute the input value function $v_x: \\vec{X} \\rightarrow \\mathbb{R}$. \n",
    "\n",
    "$$v_x(\\vec{x}) = \\mathbb{E}_{\\vec{k} \\in \\vec{K}}[q(\\vec{x}, \\vec{k}, \\pi^*(\\vec{x}, \\vec{k}))]$$\n",
    "\n",
    "This can be computed over discretized shock distribution $\\mathbf{P_K}$, which includes probability mass values for each point. This step takes time relative to the size of the discretization, $O(|\\mathbf{X}||\\mathbf{K}|)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45c608",
   "metadata": {},
   "source": [
    "#### Marginal value backup\n",
    "\n",
    "**TODO**\n",
    "\n",
    "Getting $v'_x$ from $v'_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4d777",
   "metadata": {},
   "source": [
    "#### Analytic marginal value from marginal reward\n",
    "\n",
    "See notebook \"The Envelope Theorem for Abstract Bellman Stages\" for the derivation.\n",
    "\n",
    "Using the Envelope Theorem, it is possible to derive an expression for the marginal function using the partial derivatives of the transition function and marginal reward function.\n",
    "\n",
    "Under the following conditions:\n",
    "\n",
    "- The conditions of the FOC policy solution, described above.\n",
    "- $B$ is a constant\n",
    " - A reward function such that $\\frac{\\partial F}{\\partial \\vec{x}} = 0$\n",
    " - A discount factor such that $\\frac{\\partial B}{\\partial x} = 0$.\n",
    "\n",
    "Then:\n",
    "\n",
    "$$v'_x(\\vec{x}) = \\mathbb{E}\\left[- \\frac{T^x(\\vec{x}, \\vec{k}, \\pi^*(\\vec{x}, \\vec{k}))}{T^a(\\vec{x}, \\vec{k}, \\pi^*(\\vec{x}, \\vec{k}))} F^a(\\vec{x}, \\vec{k}, \\pi^*(\\vec{x}, \\vec{k})) \\right]$$\n",
    "\n",
    "This method works in generality if the analytic expressions for the partial derivatives are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7793f1e",
   "metadata": {},
   "source": [
    "#### Endogenous Gridpoint Method\n",
    "\n",
    "Given:\n",
    " - an invertible transition function so that $T^{-1}_a: \\vec{Y} \\times \\vec{A} \\rightarrow \\vec{X}  $ is well defined\n",
    "   - No shocks allowed!\n",
    " - The optimal policy with respect to the output $\\pi^*_y$. (See EG Policy optimization above).\n",
    " \n",
    "Then we can create an endogenous grid $\\mathbf{X} = T^{-1}_a(\\mathbf{Y}, \\pi^*_y(\\mathbf{Y}))$. This this is $O(|Y|)$ with a low constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5aafa",
   "metadata": {},
   "source": [
    "## Special Points, and Interpolation\n",
    "\n",
    "Interpolation brings with it a host of challenges when the underlying functions are very curved.\n",
    "\n",
    "For example, with a CRRA utility function the utility of consuming $0$ resources is $-\\infty$. This means both that linear extrapolation from low-but-positive values will be too high, and that when an agent has no choice but to consume 0 resources they will be impossibly miserable.\n",
    "\n",
    "For this reason we offer a few tricks:\n",
    "\n",
    "### Transformed value function interpolation\n",
    "\n",
    "Rather than requiring users to use a linearly interpolated value function $v_x \\sim i(\\vec{v_x})$, we allow the user to define a transform function $f$ and its inverse $f^{-1}$ such that $v_x \\sim f^{-1}(i(f(\\vec{v_x})))$.\n",
    "\n",
    "Commonly, the transformation function $f$ is the inverse of the CRRA utility function, which is $e^x$ when $\\rho = 1$. \n",
    "\n",
    "Likewise, for the marginal value function $v'_x$, a transformation can be provided so that the interpolation is on a more linear function. In this case, with CRRA utility, the transformation is often $g(u) = u^{-\\frac{1}{\\rho}} \\sim u^{-1}$ when $\\rho = 1$.\n",
    "\n",
    "### Solution Points\n",
    "\n",
    "When it is easy to determine the optimal policy $\\pi^*$ or value function $v_x$ for a particular state $x^*$ analytically, but difficult to solve it using optimization, it is useful to input that value directly in the stage definition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hark-env",
   "language": "python",
   "name": "hark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
