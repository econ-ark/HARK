{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction in [Bayer and Luetticke (2018)](https://cepr.org/active/publications/discussion_papers/dp.php?dpno=13071)\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/econ-ark/HARK/BayerLuetticke?filepath=notebooks%2FHARK%2FBayerLuetticke%2FTwoAsset.ipynb)\n",
    "\n",
    "- Based on original slides by Christian Bayer and Ralph Luetticke \n",
    "- Original Jupyter notebook by Seungcheol Lee \n",
    "- Further edits by Chris Carroll, Tao Wang \n",
    "\n",
    "This is an accompany to the [main notebook](TwoAsset.ipynb) illustrating dimension reduction in Bayer/Luetticke algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     6,
     17
    ]
   },
   "outputs": [],
   "source": [
    "# Setup stuff\n",
    "\n",
    "# This is a jupytext paired notebook that autogenerates a corresponding .py file\n",
    "# which can be executed from a terminal command line via \"ipython [name].py\"\n",
    "# But a terminal does not permit inline figures, so we need to test jupyter vs terminal\n",
    "# Google \"how can I check if code is executed in the ipython notebook\"\n",
    "def in_ipynb():\n",
    "    try:\n",
    "        if str(type(get_ipython())) == \"<class 'ipykernel.zmqshell.ZMQInteractiveShell'>\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "# Determine whether to make the figures inline (for spyder or jupyter)\n",
    "# vs whatever is the automatic setting that will apply if run from the terminal\n",
    "if in_ipynb():\n",
    "    # %matplotlib inline generates a syntax error when run from the shell\n",
    "    # so do this instead\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline') \n",
    "else:\n",
    "    get_ipython().run_line_magic('matplotlib', 'auto') \n",
    "    \n",
    "# The tools for navigating the filesystem\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Find pathname to this file:\n",
    "my_file_path = os.path.dirname(os.path.abspath(\"TwoAsset.ipynb\"))\n",
    "\n",
    "# Relative directory for pickled code\n",
    "code_dir = os.path.join(my_file_path, \"BayerLuetticke_code/TwoAssetCode\") \n",
    "\n",
    "sys.path.insert(0, code_dir)\n",
    "sys.path.insert(0, my_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Change working folder and load Stationary equilibrium (StE)\n",
    "\n",
    "import pickle\n",
    "os.chdir(code_dir) # Go to the directory with pickled code\n",
    "\n",
    "## EX3SS_20.p is the information in the stationary equilibrium \n",
    "## (20: the number of illiquid and liquid weath gridpoints)\n",
    "### The comments above are original, but it seems that there are 30 not 20 points now\n",
    "\n",
    "EX3SS=pickle.load(open(\"EX3SS_20.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is stored in the StE?\n",
      "Grids of state variables:\n",
      "30 gridpoints for liquid assets;\n",
      "30 gridpoints for illiquid assets;\n",
      "4 gridpoints for individual productivity.\n",
      "Therefore, the joint distribution across different states has dimension of (30, 30, 4)\n",
      "The dimension of the value function for capital Vk is (30, 30, 4)\n",
      "These happen to be the same size, but need not be\n",
      "c_n is the policy function for a nonadjuster\n",
      "c_a is the policy function for an adjuster\n",
      "c_n dimensions happens to be equal to the dimensions of the grid of state variables (30, 30, 4)\n",
      "The same is true for the policy function under adjustment c_a:(30, 30, 4)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction via discrete cosine transformation and a fixed copula\n",
    "\n",
    "#### What is it whose dimension needs to be reduced?\n",
    "\n",
    "1. Policy and value functions\n",
    "1. The distribution of agents across states\n",
    "\n",
    "Grids are constructed for values of the state variables:\n",
    "  * liquid ($nm$ points), illiquid assets ($nk$), and idiosyncratic pty ($nh$)\n",
    "\n",
    "So there are $nm \\times nk \\times nh$ potential combinations\n",
    "\n",
    "In principle, functions are represented by specifying their values at each specified combination of gridpoints and interpolating for intermediate values\n",
    "  * In practice, for technical reasons, interpolation is not necessary here\n",
    "\n",
    "There are two kinds of functions:\n",
    "1. Policy functions and marginal value functions\n",
    "   * At each of the gridpoints, there is a number\n",
    "      * This is value for the value function\n",
    "      * This is consumption for the consumption function\n",
    "   * $c_n$ is the consumption function for the nonadjuster\n",
    "   * $c_a$ is the consumption function for the adjuster\n",
    "1. The distribution (=\"histograms\") of agents across states\n",
    "   * In principle, distributions need not be computed at the same gridpoints used to represent the value and policy functions\n",
    "   * In practice, the same grids are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_n is of dimension: (30, 30, 4)\n",
      "c_a is of dimension: (30, 30, 4)\n",
      "Vk is of dimension:(30, 30, 4)\n",
      "Vm is of dimension:(30, 30, 4)\n",
      "For convenience, these are all constructed from the same exogenous grids:\n",
      "30 gridpoints for liquid assets;\n",
      "30 gridpoints for illiquid assets;\n",
      "4 gridpoints for individual productivity.\n",
      "\n",
      "Therefore, the joint distribution across different is of size: \n",
      "30 * 30 * 4 = 3600\n"
     ]
    }
   ],
   "source": [
    "# Recover dimensions of the marginal value and consumption functions\n",
    "\n",
    "print('c_n is of dimension: ' + str(EX3SS['mutil_c_n'].shape))\n",
    "print('c_a is of dimension: ' + str(EX3SS['mutil_c_a'].shape))\n",
    "\n",
    "print('Vk is of dimension:' + str(EX3SS['Vk'].shape))\n",
    "print('Vm is of dimension:' + str(EX3SS['Vm'].shape))\n",
    "\n",
    "print('For convenience, these are all constructed from the same exogenous grids:')\n",
    "print(str(len(EX3SS['grid']['m']))+' gridpoints for liquid assets;')\n",
    "print(str(len(EX3SS['grid']['k']))+' gridpoints for illiquid assets;')\n",
    "print(str(len(EX3SS['grid']['h']))+' gridpoints for individual productivity.')\n",
    "print('')\n",
    "print('Therefore, the joint distribution across different is of size: ')\n",
    "print(str(EX3SS['mpar']['nm'])+\n",
    "      ' * '+str(EX3SS['mpar']['nk'])+\n",
    "      ' * '+str(EX3SS['mpar']['nh'])+\n",
    "      ' = '+ str(EX3SS['mpar']['nm']*EX3SS['mpar']['nk']*EX3SS['mpar']['nh']))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuitively, how does the reduction work?\n",
    "\n",
    "- The first step is to find an efficient \"compressed\" representation of the function (e.g., the consumption function).  The analogy to image compression is that nearby pixels are likely to have identical or very similar colors, so we need only to find an efficient way to represent the way in which the colors change from one pixel to another.  Similarly, consumption at a given point is likely to be close to consumption at a nearby point, so a function that captures that similarity efficiently can preserve most of the information without keeping all of the points.\n",
    "\n",
    "- We will be using the discrete consine transformation (DCT), which is commonly used in image compression. See [here](https://en.wikipedia.org/wiki/Discrete_cosine_transform) for the Wikipedia page on DCT. \n",
    "\n",
    "- The other tool we use is the \"copula,\" which allows us to represent the distribution of people across idiosyncratic states efficiently\n",
    "   * The crucial assumption behind the copula is that what aggregate shocks do is to squeeze or distort the steady state distribution, but leave the rank structure of the distribution the same. Think of representing a balloon by a set of points on its surface; the copula assumption is effectively that when something happens to the balloon (more air is put in it, or it is squeezed on one side, say), we can represent what happens by thinking about how the relationship between points is distorted, rather than having to reconstruct the shape of the balloon with a completely independent set of new points.  Which points are close to which other points does not change, but the distances between them can change.  If the distances between them change in a particularly simple way, you can represent what has happened with a small amount of information.  For example, if the balloon is perfectly spherical, then adding a given amount of air might increase the distances between adjacent points by 5 percent.  (See the video illustration here)\n",
    "\n",
    "- In the context of this model, the assumption is that the rank order correlation (e.g. the correlation of where you are in the distribution of liquid assets and illiquid assets) remains the same after the aggregate shocks are introduced to StE\n",
    "\n",
    "- In this case we just need to represent how the marginal distributions of each state change, instead of the full joint distributions. \n",
    "\n",
    "- This reduces 3600 $\\times$ 3 to 30+30+4=64. See [here](https://en.wikipedia.org/wiki/Copula_(probability_theory)) for the Wikipedia page on copula.\n",
    "\n",
    "(Eliminate or rewrite intuitively the stuff below)\n",
    "\n",
    "#### More accurately, how?\n",
    "1. Use compression techniques as in video encoding\n",
    "   * Apply a discrete cosine transformation (DCT) to all value/policy functions\n",
    "      * Use Chebychev polynomials on roots grid \n",
    "   * Define a reference \"frame\": the steady-state equilibrium (StE)\n",
    "   * Represent fluctuations as differences from this reference frame\n",
    "   * Assume all coefficients of the DCT from the StE that are close to zero do not change when there is an aggregate shock (small things stay small and unchanged)\n",
    "   \n",
    "2. Assume no changes in the rank correlation structure of $\\mu$   \n",
    "   * Calculate the Copula, $\\bar{C}$ of $\\mu$ in the StE\n",
    "   * Perturb only the marginal distributions\n",
    "   * Use fixed Copula to calculate an approximate joint distribution from marginals\n",
    "\n",
    "\n",
    "The approach follows the insight of KS in that it uses the fact that some moments of the distribution do not matter for aggregate dynamics\n",
    "\n",
    "The copula is computed from the joint distribution of states in StE and will be used to transform the marginals back to joint distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Get some specs about the copula, which is precomputed in the EX3SS object\n",
    "\n",
    "print('The copula consists of two parts: gridpoints and values at those gridpoints:'+ \\\n",
    "      ',\\n gridpoints with dimension of '+str(EX3SS['Copula']['grid'].shape) + \\\n",
    "      ', where the first element is total number of gridpoints' + \\\n",
    "      ', and the second element is number of states' + \\\n",
    "     ',\\n and values with dimension of '+str(EX3SS['Copula']['value'].shape) + \\\n",
    "      ', \\n each entry of which is the probability of the three state variables below the grids.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Import necessary libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys \n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import matrix_rank\n",
    "import scipy as sc\n",
    "from scipy.stats import norm \n",
    "from scipy.interpolate import interp1d, interp2d, griddata, RegularGridInterpolator, interpn\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, cpu_count, Process\n",
    "from math import ceil\n",
    "import math as mt\n",
    "from scipy import sparse as sp  # used to work with sparse matrices\n",
    "from scipy import linalg   #linear algebra \n",
    "from math import log, cos, pi, sqrt\n",
    "import time\n",
    "from SharedFunc3 import Transition, ExTransitions, GenWeight, MakeGridkm, Tauchen, Fastroot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import scipy.io #scipy input and output\n",
    "import scipy.fftpack as sf  # scipy discrete fourier transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details\n",
    "1) Apply compression techniques from video encoding\n",
    "   * Let $\\bar{\\Theta} = dct(\\bar{v})$ be the coefficients obtained from the DCT of the value function in StE\n",
    "   * Define an index set $\\mathop{I}$ that contains the x percent largest (i.e. most important) elements from $\\bar{\\Theta}$\n",
    "   * Let $\\theta$ be a sparse vector with non-zero entries only for elements $i \\in \\mathop{I}$\n",
    "   * Define \n",
    "   \\begin{equation}\n",
    "    \\tilde{\\Theta}(\\theta_t)=\\left\\{\n",
    "      \\begin{array}{@{}ll@{}}\n",
    "         \\bar{\\Theta}(i)+\\theta_t(i), & i \\in \\mathop{I} \\\\\n",
    "         \\bar{\\Theta}(i), & \\text{else}\n",
    "      \\end{array}\\right.\n",
    "   \\end{equation}\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Decoding\n",
    "   * Now we reconstruct $\\tilde{v}_t=\\tilde{v}(\\theta_t)=dct^{-1}(\\tilde{\\Theta}(\\theta_i))$\n",
    "      * idct is the inverse dct that goes from the $\\theta$ vector to the corresponding values\n",
    "   * This means that in the StE the reduction step adds no addtional approximation error:\n",
    "       * Remember that $\\tilde{v}(0)=\\bar{v}$ by construction\n",
    "   * Yet, it allows to reduce the number of derivatives that need to be calculated from the outset.\n",
    "   \n",
    "3) The histogram is recovered the same way\n",
    "   * $\\mu_t$ is approximated as $\\bar{C}(\\bar{\\mu_t}^1,...,\\bar{\\mu_t}^n)$, where $n$ is the dimensionality of the idiosyncratic states\n",
    "   * The StE distribution is obtained when $\\mu = \\bar{C}(\\bar{\\mu}^1,...,\\bar{\\mu}^n)$\n",
    "   * Typically prices are only influenced through the marginal distributions\n",
    "   * The approach ensures that changes in the mass of one, say wealth, state are distributed in a sensible way across the other dimension\n",
    "   * The implied distributions look \"similar\" to the StE one (different in (Reiter, 2009))\n",
    "\n",
    "4) Too many equations\n",
    "   * The system\n",
    "     \\begin{align}\n",
    "      F(\\{d\\mu_t^1,...,d\\mu_t^n\\}, S_t, \\{d\\mu_{t+1}^1,...,d\\mu_{t+1}^n\\}, S_{t+1}, \\theta_t, P_t, \\theta_{t+1}, P_{t+1})\n",
    "      &= \\begin{bmatrix}\n",
    "           d\\bar{C}(\\bar{\\mu}_t^1,...,\\bar{\\mu}_t^n) - d\\bar{C}(\\bar{\\mu}_t^1,...,\\bar{\\mu}_t^n)\\Pi_{h_t} \\\\\n",
    "           dct[idct(\\tilde{\\Theta(\\theta_t)}) - (\\bar{u}_{h_t} + \\beta \\Pi_{h_t}idct(\\tilde{\\Theta(\\theta_{t+1})}] \\\\\n",
    "           S_{t+1} - H(S_t,d\\mu_t) \\\\\n",
    "           \\Phi(h_t,d\\mu_t,P_t,S_t) \\\\\n",
    "           \\end{bmatrix}\n",
    "     \\end{align}\n",
    "     has too many equations\n",
    "   * Uses only difference in marginals and the differences on $\\mathop{I}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## State reduction and discrete cosine transformation\n",
    "\n",
    "class StateReduc_Dct:\n",
    "    \n",
    "    def __init__(self, par, mpar, grid, Output, targets, Vm, Vk, \n",
    "                 joint_distr, Copula, c_n_guess, c_a_guess, psi_guess,\n",
    "                 m_n_star, m_a_star, cap_a_star, mutil_c_n, mutil_c_a,mutil_c, P_H):\n",
    "         \n",
    "        self.par = par         # Parameters of the theoretical model\n",
    "        self.mpar = mpar       # Parameters of the numerical representation\n",
    "        self.grid = grid       # Discrete grid\n",
    "        self.Output = Output   # Results of the calculations\n",
    "        self.targets = targets # Like, debt-to-GDP ratio or other desiderata\n",
    "        self.Vm = Vm           # Marginal value from liquid cash-on-hand\n",
    "        self.Vk = Vk           # Marginal value of capital\n",
    "        self.joint_distr = joint_distr # Multidimensional histogram\n",
    "        self.Copula = Copula   # Encodes rank correlation structure of distribution\n",
    "        self.mutil_c = mutil_c # Marginal utility of consumption\n",
    "        self.P_H = P_H         # Transition matrix for macro states (not including distribution)\n",
    "        \n",
    "        \n",
    "    def StateReduc(self):\n",
    "        \"\"\"\n",
    "        input\n",
    "        -----\n",
    "        self: dict, stored results from a StE \n",
    "        \n",
    "        output\n",
    "        ------\n",
    "        Newly generated\n",
    "        ===============\n",
    "        X_ss: ndarray, stacked states, including  \n",
    "        Y_ss:  ndarray, controls \n",
    "        Gamma_state: ndarray, marginal distributions of individual states \n",
    "        grid: ndarray, discrete grids\n",
    "        targets: ndarray, debt-to-GDP ratio or other desiderata\n",
    "        P_H: transition probability of\n",
    "        indexMUdct: ndarray, indices selected after dct operation on marginal utility of consumption\n",
    "        indexVKdct: ndarray, indices selected after dct operation on marginal value of capital\n",
    "        State: ndarray, dimension equal to reduced states\n",
    "        State_m: ndarray, dimension equal to reduced states\n",
    "        Contr:  ndarray, dimension equal to reduced controls\n",
    "        Contr_m: ndarray, dimension equal to reduced controls\n",
    "        \n",
    "        Passed down from the model\n",
    "        ==========================\n",
    "        Copula: dict, grids and values\n",
    "        joint_distr: ndarray, nk x nm x nh\n",
    "        Output: dict, outputs from the model \n",
    "        par: dict, parameters of the theoretical model\n",
    "        mpar:dict, parameters of the numerical representation\n",
    "        aggrshock: string, type of aggregate shock used to purturb the StE \n",
    "        \"\"\"\n",
    "        \n",
    "        # Inverse of CRRA on x for utility and marginal utility\n",
    "        invutil = lambda x : ((1-self.par['xi'])*x)**(1./(1-self.par['xi'])) \n",
    "        invmutil = lambda x : (1./x)**(1./self.par['xi'])                    \n",
    "            \n",
    "        # X=States\n",
    "        # Marg dist of liquid assets summing over pty and illiquid assets k\n",
    "        Xss=np.asmatrix(np.concatenate((np.sum(np.sum(self.joint_distr.copy(),axis=1),axis =1),  \n",
    "                       np.transpose(np.sum(np.sum(self.joint_distr.copy(),axis=0),axis=1)),# marg dist k\n",
    "                       np.sum(np.sum(self.joint_distr.copy(),axis=1),axis=0), # marg dist pty (\\approx income)\n",
    "                       [np.log(self.par['RB'])],[ 0.]))).T # Given the constant interest rate\n",
    "        \n",
    "        # Y=\"controls\" (according to this literature's odd terminology)\n",
    "        # c = invmarg(marg(c)), so first bit gets consumption policy function\n",
    "        Yss=np.asmatrix(np.concatenate((invmutil(self.mutil_c.copy().flatten(order = 'F')),\\\n",
    "                                        invmutil(self.Vk.copy().flatten(order = 'F')),\n",
    "                      [np.log(self.par['Q'])], # Question: Price of the illiquid asset, right?\n",
    "                                        [ np.log(self.par['PI'])], # Inflation\n",
    "                                        [ np.log(self.Output)],    \n",
    "                      [np.log(self.par['G'])], # Gov spending\n",
    "                                        [np.log(self.par['W'])], # Wage\n",
    "                                        [np.log(self.par['R'])], # Nominal R\n",
    "                                        [np.log(self.par['PROFITS'])], \n",
    "                      [np.log(self.par['N'])], # Hours worked\n",
    "                                        [np.log(self.targets['T'])], # Taxes\n",
    "                                        [np.log(self.grid['K'])],    # Kapital\n",
    "                      [np.log(self.targets['B'])]))).T # Government debt\n",
    "        \n",
    "        # Mapping for Histogram\n",
    "        # Gamma_state matrix reduced set of states\n",
    "        #   nm = number of gridpoints for liquid assets\n",
    "        #   nk = number of gridpoints for illiquid assets\n",
    "        #   nh = number of gridpoints for human capital (pty)\n",
    "        Gamma_state = np.zeros( # Create zero matrix of size [nm + nk + nh,nm + nk + nh - 4]\n",
    "            (self.mpar['nm']+self.mpar['nk']+self.mpar['nh'],\n",
    "             self.mpar['nm']+self.mpar['nk']+self.mpar['nh'] - 4)) \n",
    "            # Question: Why 4? 4 = 3+1, 3: sum to 1 for m, k, h and 1: for entrepreneurs \n",
    "\n",
    "        # Impose adding-up conditions: \n",
    "        # In each of the block matrices, probabilities must add to 1\n",
    "        \n",
    "        for j in range(self.mpar['nm']-1): # np.squeeze reduces one-dimensional matrix to vector\n",
    "            Gamma_state[0:self.mpar['nm'],j] = -np.squeeze(Xss[0:self.mpar['nm']])\n",
    "            Gamma_state[j,j]=1. - Xss[j]   #   \n",
    "            Gamma_state[j,j]=Gamma_state[j,j] - np.sum(Gamma_state[0:self.mpar['nm'],j])\n",
    "        bb = self.mpar['nm'] # Question: bb='bottom base'? because bb shorter to type than self.mpar['nm'] everywhere\n",
    "\n",
    "        for j in range(self.mpar['nk']-1):\n",
    "            Gamma_state[bb+np.arange(0,self.mpar['nk'],1), bb+j-1] = -np.squeeze(Xss[bb+np.arange(0,self.mpar['nk'],1)])\n",
    "            Gamma_state[bb+j,bb-1+j] = 1. - Xss[bb+j] \n",
    "            Gamma_state[bb+j,bb-1+j] = (Gamma_state[bb+j,bb-1+j] - \n",
    "                                        np.sum(Gamma_state[bb+np.arange(0,self.mpar['nk']),bb-1+j]))\n",
    "        bb = self.mpar['nm'] + self.mpar['nk']\n",
    "\n",
    "        for j in range(self.mpar['nh']-2): \n",
    "            # Question: Why -2?  1 for h sum to 1 and 1 for entrepreneur  Some other symmetry/adding-up condition.\n",
    "            Gamma_state[bb+np.arange(0,self.mpar['nh']-1,1), bb+j-2] = -np.squeeze(Xss[bb+np.arange(0,self.mpar['nh']-1,1)])\n",
    "            Gamma_state[bb+j,bb-2+j] = 1. - Xss[bb+j]\n",
    "            Gamma_state[bb+j,bb-2+j] = Gamma_state[bb+j,bb-2+j] - np.sum(Gamma_state[bb+np.arange(0,self.mpar['nh']-1,1),bb-2+j])\n",
    "\n",
    "        # Number of other state variables not including the gridded -- here, just the interest rate \n",
    "        self.mpar['os'] = len(Xss) - (self.mpar['nm']+self.mpar['nk']+self.mpar['nh'])\n",
    "        # For each gridpoint there are two \"regular\" controls: consumption and illiquid saving\n",
    "        # Counts the number of \"other\" controls (PROFITS, Q, etc)\n",
    "        self.mpar['oc'] = len(Yss) - 2*(self.mpar['nm']*self.mpar['nk']*self.mpar['nh'])\n",
    "        \n",
    "        aggrshock = self.par['aggrshock']\n",
    "        accuracy = self.par['accuracy']\n",
    "       \n",
    "        # Do the dct on the steady state marginal utility\n",
    "        # Returns an array of indices for the used basis vectors\n",
    "        indexMUdct = self.do_dct(invmutil(self.mutil_c.copy().flatten(order='F')),\n",
    "                                 self.mpar,accuracy)\n",
    "\n",
    "        # Do the dct on the steady state marginal value of capital\n",
    "        # Returns an array of indices for the used basis vectors\n",
    "        indexVKdct = self.do_dct(invmutil(self.Vk.copy()),self.mpar,accuracy)\n",
    "                \n",
    "        # Calculate the numbers of states and controls\n",
    "        aux = np.shape(Gamma_state)\n",
    "        self.mpar['numstates'] = np.int64(aux[1] + self.mpar['os'])\n",
    "        self.mpar['numcontrols'] = np.int64(len(indexMUdct) + \n",
    "                                            len(indexVKdct) + \n",
    "                                            self.mpar['oc'])\n",
    "        \n",
    "        # Size of the reduced matrices to be used in the Fsys\n",
    "        # Set to zero because in steady state they are zero\n",
    "        State = np.zeros((self.mpar['numstates'],1))\n",
    "        State_m = State\n",
    "        Contr = np.zeros((self.mpar['numcontrols'],1))\n",
    "        Contr_m = Contr\n",
    "        \n",
    "        return {'Xss': Xss, 'Yss':Yss, 'Gamma_state': Gamma_state, \n",
    "                'par':self.par, 'mpar':self.mpar, 'aggrshock':aggrshock,\n",
    "                'Copula':self.Copula,'grid':self.grid,'targets':self.targets,'P_H':self.P_H, \n",
    "                'joint_distr': self.joint_distr, 'Output': self.Output, 'indexMUdct':indexMUdct, 'indexVKdct':indexVKdct,\n",
    "                'State':State, 'State_m':State_m, 'Contr':Contr, 'Contr_m':Contr_m}\n",
    "\n",
    "    # Discrete cosine transformation magic happens here\n",
    "    # sf is scipy.fftpack tool\n",
    "    def do_dct(self, obj, mpar, level):\n",
    "        \"\"\"\n",
    "        input\n",
    "        -----\n",
    "        obj: ndarray nm x nk x nh  \n",
    "             dimension of states before dct \n",
    "        mpar: dict\n",
    "            parameters in the numerical representaion of the model, e.g. nm, nk and nh\n",
    "        level: float \n",
    "               accuracy level for dct \n",
    "        output\n",
    "        ------\n",
    "        index_reduced: ndarray n_dct x 1 \n",
    "                       an array of indices that select the needed grids after dct\n",
    "                   \n",
    "        \"\"\"\n",
    "        obj = np.reshape(obj.copy(),(mpar['nm'],mpar['nk'],mpar['nh']),order='F')\n",
    "        X1 = sf.dct(obj,norm='ortho',axis=0)    # dct is operated along three dimensions axis=0/1/2\n",
    "        X2 = sf.dct(X1.copy(),norm='ortho',axis=1)\n",
    "        X3 = sf.dct(X2.copy(),norm='ortho',axis=2)\n",
    "\n",
    "        # Pick the coefficients that are big\n",
    "        XX = X3.flatten(order='F') \n",
    "        ind = np.argsort(abs(XX.copy()))[::-1]\n",
    "        #  i will \n",
    "        i = 1    \n",
    "        # Sort from smallest (=best) to biggest (=worst)\n",
    "        # and count how many are 'good enough to keep'\n",
    "        while linalg.norm(XX[ind[:i]].copy())/linalg.norm(XX) < level:\n",
    "              i += 1    \n",
    "        \n",
    "        needed = i # Question:Isn't this counting the ones that are NOT needed?\n",
    "        \n",
    "        index_reduced = np.sort(ind[:i]) # Retrieve the good \n",
    "        \n",
    "        return index_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Choose an aggregate shock to perturb(one of three shocks: MP, TFP, Uncertainty)\n",
    "\n",
    "EX3SS['par']['aggrshock']           = 'MP'\n",
    "EX3SS['par']['rhoS']    = 0.0      # Persistence of variance\n",
    "EX3SS['par']['sigmaS']  = 0.001    # STD of variance shocks\n",
    "\n",
    "#EX3SS['par']['aggrshock']           = 'TFP'\n",
    "#EX3SS['par']['rhoS']    = 0.95\n",
    "#EX3SS['par']['sigmaS']  = 0.0075\n",
    "    \n",
    "#EX3SS['par']['aggrshock']           = 'Uncertainty'\n",
    "#EX3SS['par']['rhoS']    = 0.84    # Persistence of variance\n",
    "#EX3SS['par']['sigmaS']  = 0.54    # STD of variance shocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Choose an accuracy of approximation with DCT\n",
    "### Determines number of basis functions chosen -- enough to match this accuracy\n",
    "### EX3SS is precomputed steady-state pulled in above\n",
    "EX3SS['par']['accuracy'] = 0.99999 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Implement state reduction and DCT\n",
    "### Do state reduction on steady state\n",
    "EX3SR=StateReduc_Dct(**EX3SS)   # Takes StE result as input and get ready to invoke state reduction operation\n",
    "SR=EX3SR.StateReduc()           # StateReduc is operated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the results from the state reduction?\n",
      "Newly added attributes after the operation include \n",
      "{'indexMUdct', 'State', 'Contr', 'Xss', 'Contr_m', 'aggrshock', 'Yss', 'State_m', 'indexVKdct', 'Gamma_state'}\n",
      "\n",
      "\n",
      "The dimension of policy function is reduced to 154 from (30, 30, 4)\n",
      "The dimension of value function is reduced to 94 from (30, 30, 4)\n",
      "The total number of control variables is 259=154+94+ # of other macro controls\n",
      "\n",
      "\n",
      "After marginalizing the joint-distribution,       \n",
      " the dimension of states including exogenous state, is 66\n",
      "Dimension of gamma_state is (64, 60). It simply stacks all grids of different      \n",
      " state variables regardless of their joint distributions.      \n",
      " This is due to the assumption of the rank order remains the same.\n",
      "The total number of state variables is 62=60+ # of other states\n"
     ]
    }
   ],
   "source": [
    "print('What are the results from the state reduction?')\n",
    "print('Newly added attributes after the operation include \\n'+str(set(SR.keys())-set(EX3SS.keys())))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('The dimension of policy function is reduced to '+str(SR['indexMUdct'].shape[0]) \\\n",
    "      +' from '+str(EX3SS['mutil_c'].shape))\n",
    "print('The dimension of value function is reduced to '+str(SR['indexVKdct'].shape[0]) \\\n",
    "      + ' from ' + str(EX3SS['Vk'].shape))\n",
    "print('The total number of control variables is '+str(SR['Contr'].shape[0])+'='+str(SR['indexMUdct'].shape[0]) + \\\n",
    "      '+'+str(SR['indexVKdct'].shape[0])+'+ # of other macro controls')\n",
    "print('\\n')\n",
    "print('After marginalizing the joint distribution, \\\n",
    "      \\n the dimension of states including exogenous state, is '+str(SR['Xss'].shape[0]))\n",
    "print('Dimension of gamma_state is '+str(SR['Gamma_state'].shape)+\\\n",
    "      '. It simply stacks all grids of different\\\n",
    "      \\n state variables regardless of their joint distributions.\\\n",
    "      \\n This is due to the assumption of the rank order remains the same.')\n",
    "print('The total number of state variables is '+str(SR['State'].shape[0]) + '='+\\\n",
    "     str(SR['Gamma_state'].shape[1])+'+ # of other states')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: what do we achieve after the transformation?\n",
    "\n",
    "- Via DCT, the dimension of policy function and value functions are reduced both from 3600 to 154 and 94, respectively.\n",
    "- Via fixed copula operation and marginalizing the joint-distribution, the dimension of gamma_state is 64 now, (excluding exogeous states like interest rate)."
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6202365/L5GBWHBM": {
     "author": [
      {
       "family": "Reiter",
       "given": "Michael"
      }
     ],
     "container-title": "Journal of Economic Dynamics and Control",
     "id": "undefined",
     "issue": "1",
     "issued": {
      "month": 1,
      "year": 2010
     },
     "note": "Citation Key: reiterBackward",
     "page": "28-35",
     "page-first": "28",
     "title": "Solving the Incomplete Markets Model with Aggregate Uncertainty by Backward Induction",
     "type": "article-journal",
     "volume": "34"
    },
    "6202365/UKUXJHCN": {
     "author": [
      {
       "family": "Reiter",
       "given": "Michael"
      }
     ],
     "id": "6202365/UKUXJHCN",
     "note": "Citation Key: reiter2002recursive \nbibtex*[publisher=Citeseer]",
     "title": "Recursive computation of heterogeneous agent models",
     "type": "article-journal"
    },
    "6202365/VPUXICUR": {
     "author": [
      {
       "family": "Krusell",
       "given": "Per"
      },
      {
       "family": "Smith",
       "given": "Anthony A."
      }
     ],
     "container-title": "Journal of Political Economy",
     "id": "6202365/VPUXICUR",
     "issue": "5",
     "issued": {
      "year": 1998
     },
     "page": "867â€“896",
     "page-first": "867",
     "title": "Income and Wealth Heterogeneity in the Macroeconomy",
     "type": "article-journal",
     "volume": "106"
    },
    "6202365/WN76AW6Q": {
     "author": [
      {
       "family": "SeHyoun Ahn, Greg Kaplan, Benjamin Moll, Thomas Winberry",
       "given": ""
      },
      {
       "family": "Wolf",
       "given": "Christian"
      }
     ],
     "editor": [
      {
       "family": "Parker",
       "given": "Jonathan"
      },
      {
       "family": "Martin S. Eichenbaum",
       "given": "Organizers"
      }
     ],
     "id": "6202365/WN76AW6Q",
     "issued": {
      "year": 2017
     },
     "note": "Citation Key: akmwwInequality \nbibtex*[booktitle=NBER Macroeconomics Annual;publisher=MIT Press;location=Cambridge, MA]",
     "title": "When Inequality Matters for Macro and Macro Matters for Inequality",
     "type": "article-journal",
     "volume": "32"
    },
    "undefined": {
     "author": [
      {
       "family": "Reiter",
       "given": "Michael"
      }
     ],
     "container-title": "Journal of Economic Dynamics and Control",
     "id": "undefined",
     "issue": "1",
     "issued": {
      "month": 1,
      "year": 2010
     },
     "note": "Citation Key: reiterBackward",
     "page": "28-35",
     "page-first": "28",
     "title": "Solving the Incomplete Markets Model with Aggregate Uncertainty by Backward Induction",
     "type": "article-journal",
     "volume": "34"
    }
   }
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
